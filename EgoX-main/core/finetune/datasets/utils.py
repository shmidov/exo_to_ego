import logging
from pathlib import Path
from typing import List, Tuple

import cv2
import torch
import imageio
import numpy as np
from torchvision.transforms.functional import resize
from safetensors.torch import load

# Must import after torch because this can sometimes lead to a nasty segmentation fault, or stack smashing error
# Very few bug reports but it happens. Look in decord Github issues for more relevant information.
import decord  # isort:skip
decord.bridge.set_bridge("torch")

def generate_uniform_pointmap(height, width):
    x = np.linspace(-1, 1, width)
    y = np.linspace(-1, 1, height)
    xv, yv = np.meshgrid(x, y)
    
    # Create a spatially varying Z value: increases from bottom (row 0) to top (row height-1)
    zv = 1 - np.linspace(0, 1, height)[:, None]  # shape (H, 1)
    zv = np.repeat(zv, width, axis=1)        # shape (H, W)
    
    # Stack to get (H, W, 3) array
    pointmap = np.stack([xv, yv, zv], axis=-1)
    
    # Normalize XYZ to [0, 1] for image saving
    # X and Y are already in [-1, 1], so map to [0, 1]
    pointmap[..., 0] = (pointmap[..., 0] + 1) / 2
    pointmap[..., 1] = (pointmap[..., 1] + 1) / 2
    # Z is constant, but ensure it's in [0, 1]
    pointmap[..., 2] = (pointmap[..., 2] - np.min(pointmap[..., 2])) / (np.ptp(pointmap[..., 2]) + 1e-8)
    return pointmap

##########  loaders  ##########

def load_safetensors(model_path: Path) -> torch.Tensor:
    with open(model_path, "rb") as model:
        data = model.read()
        loaded = load(data)
    return loaded

def load_prompts(prompt_path: Path) -> List[str]:
    with open(prompt_path, "r", encoding="utf-8") as file:
        return [line.strip() for line in file.readlines() if len(line.strip()) > 0]


def load_videos(video_path: Path) -> List[Path]:
    with open(video_path, "r", encoding="utf-8") as file:
        return [video_path.parent / line.strip() for line in file.readlines() if len(line.strip()) > 0]


def load_images(image_path: Path) -> List[Path]:
    with open(image_path, "r", encoding="utf-8") as file:
        return [image_path.parent / line.strip() for line in file.readlines() if len(line.strip()) > 0]


def load_images_from_videos(videos_path: List[Path]) -> List[Path]:
    first_frames_dir = videos_path[0].parent.parent / "first_frames"
    first_frames_dir.mkdir(exist_ok=True)

    first_frame_paths = []
    for video_path in videos_path:
        frame_path = first_frames_dir / f"{video_path.stem}.png"
        if frame_path.exists():
            first_frame_paths.append(frame_path)
            continue

        # Open video
        cap = cv2.VideoCapture(str(video_path))

        # Read first frame
        ret, frame = cap.read()
        if not ret:
            raise RuntimeError(f"Failed to read video: {video_path}")

        # Save frame as PNG with same name as video
        cv2.imwrite(str(frame_path), frame)
        logging.info(f"Saved first frame to {frame_path}")

        # Release video capture
        cap.release()

        first_frame_paths.append(frame_path)

    return first_frame_paths


def load_from_json_file(json_path: Path) -> dict:
    import json

    with open(json_path, "r", encoding="utf-8") as file:
        data = json.load(file)
        
    return data

def iproj_disp(
        intrinsics: torch.Tensor,
        disps: torch.Tensor,
        disps_u: torch.Tensor,
        disps_v: torch.Tensor,
        compute_jz: bool = False,
        compute_jf: bool = False,
    ):
        # Expand intrinsics.
        # intrinsics = intrinsics.view((-1,) + (1,) * (disps.dim() - 1) + (4,))
        fx, fy, cx, cy = intrinsics.unbind(dim=-1)

        i = torch.ones_like(disps)
        X = (disps_u - cx) / fx
        Y = (disps_v - cy) / fy
        pts = torch.stack([X, Y, i, disps], dim=-1)

        Jz = None
        if compute_jz:
            Jz = torch.zeros_like(pts)
            Jz[..., -1] = 1.0

        Jf = None
        if compute_jf:
            Jf = torch.zeros_like(pts)[..., None]
            Jf[..., 0, 0] = -X / fx
            Jf[..., 1, 0] = -Y / fy

        return pts, Jz, Jf

##########  preprocessors  ##########


def preprocess_image_with_resize(
    image_path: Path | str,
    height: int,
    width: int,
) -> torch.Tensor:
    """
    Loads and resizes a single image.

    Args:
        image_path: Path to the image file.
        height: Target height for resizing.
        width: Target width for resizing.

    Returns:
        torch.Tensor: Image tensor with shape [C, H, W] where:
            C = number of channels (3 for RGB)
            H = height
            W = width
    """
    if isinstance(image_path, str):
        image_path = Path(image_path)
    image = imageio.imread(image_path.as_posix())
    # TODO: double check why there is error when using the below pipeline to load: libpng error: bad parameters to zlib
    # image = cv2.imread(image_path.as_posix())
    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (width, height))
    image = torch.from_numpy(image).float()
    image = image.permute(2, 0, 1).contiguous()
    return image


def preprocess_video_with_resize(
    video_path: Path | str,
    max_num_frames: int,
    height: int,
    width: int,
    video_type: str,
) -> torch.Tensor:
    """
    Loads and resizes a single video for IC-LoRA training.

    The function processes the video through these steps:
      1. Determine target dimensions based on video type:
         - exo: (exo_width, height) where exo_width = width - height
         - ego_gt/ego_prior: (ego_width, height) where ego_width = height
      2. If video frame count > max_num_frames, take first max_num_frames
      3. If video frame count < max_num_frames, repeat last frame

    Args:
        video_path: Path to the video file.
        max_num_frames: Maximum number of frames to keep.
        height: Target height for resizing.
        width: Total width (exo_width + ego_width).
        video_type: Type of video ("exo", "ego_gt", or "ego_prior").

    Returns:
        A torch.Tensor with shape [F, C, H, W] where:
          F = number of frames
          C = number of channels (3 for RGB)
          H = height
          W = target width based on video type
    """
    if isinstance(video_path, str):
        video_path = Path(video_path)
    
    # Determine target dimensions based on video type
    if video_type == "exo":
        target_width = width - height  # exo_width
        video_reader = decord.VideoReader(uri=video_path.as_posix(), width=target_width, height=height)
        video_num_frames = len(video_reader)
        
        if video_num_frames < max_num_frames:
            # Get all frames first
            frames = video_reader.get_batch(list(range(video_num_frames)))
            # Repeat the last frame until we reach max_num_frames
            last_frame = frames[-1:]
            num_repeats = max_num_frames - video_num_frames
            repeated_frames = last_frame.repeat(num_repeats, 1, 1, 1)
            frames = torch.cat([frames, repeated_frames], dim=0)
            return frames.float().permute(0, 3, 1, 2).contiguous()
        else:
            # Take first max_num_frames (no evenly downsampling)
            indices = list(range(max_num_frames))
            frames = video_reader.get_batch(indices)
            frames = frames.float()
            frames = frames.permute(0, 3, 1, 2).contiguous()
            return frames
    elif video_type in ["ego_gt", "ego_prior"]:
        target_width = height  # ego_width
        video_reader = decord.VideoReader(uri=video_path.as_posix(), width=target_width, height=height)
        video_num_frames = len(video_reader)
        
        if video_num_frames < max_num_frames:
            # Get all frames first
            frames = video_reader.get_batch(list(range(video_num_frames)))
            # Repeat the last frame until we reach max_num_frames
            last_frame = frames[-1:]
            num_repeats = max_num_frames - video_num_frames
            repeated_frames = last_frame.repeat(num_repeats, 1, 1, 1)
            frames = torch.cat([frames, repeated_frames], dim=0)
            return frames.float().permute(0, 3, 1, 2).contiguous()
        else:
            # Take first max_num_frames (no evenly downsampling)
            indices = list(range(max_num_frames))
            frames = video_reader.get_batch(indices)
            frames = frames.float()
            frames = frames.permute(0, 3, 1, 2).contiguous()
            return frames
    
    elif video_type == "exo_GGA":
        height = 448
        width = 796 + 448
        target_width = width - height  # exo_width
        video_reader = decord.VideoReader(uri=video_path.as_posix(), width=width, height=height)
        video_num_frames = len(video_reader)
        
        if video_num_frames < max_num_frames:
            # Get all frames first
            frames = video_reader.get_batch(list(range(video_num_frames)))
            # Repeat the last frame until we reach max_num_frames
            last_frame = frames[-1:]
            num_repeats = max_num_frames - video_num_frames
            repeated_frames = last_frame.repeat(num_repeats, 1, 1, 1)
            frames = torch.cat([frames, repeated_frames], dim=0)
            frames = frames[:, :, :target_width, :]  # Crop to exo_width
            #! resize frames to (3, 784, 448)
            frames = resize(frames, (784, 448))
            return frames.float().permute(0, 3, 1, 2).contiguous()
        else:
            # Take first max_num_frames (no evenly downsampling)
            indices = list(range(max_num_frames))
            frames = video_reader.get_batch(indices)
            frames = frames.float()
            frames = frames[:, :, :target_width, :]  # Crop to exo_width
            frames = resize(frames, (784, 448))
            frames = frames.permute(0, 3, 1, 2).contiguous()
            return frames

    elif video_type == "ego_gt_GGA":
        height = 448
        width = 796 + 448
        target_width = width - height  # exo_width
        video_reader = decord.VideoReader(uri=video_path.as_posix(), width=width, height=height)
        video_num_frames = len(video_reader)
        
        if video_num_frames < max_num_frames:
            # Get all frames first
            frames = video_reader.get_batch(list(range(video_num_frames)))
            # Repeat the last frame until we reach max_num_frames
            last_frame = frames[-1:]
            num_repeats = max_num_frames - video_num_frames
            repeated_frames = last_frame.repeat(num_repeats, 1, 1, 1)
            frames = torch.cat([frames, repeated_frames], dim=0)
            frames = frames[:, :, target_width:, :]  # Crop to exo_width
            return frames.float().permute(0, 3, 1, 2).contiguous()
        else:
            # Take first max_num_frames (no evenly downsampling)
            indices = list(range(max_num_frames))
            frames = video_reader.get_batch(indices)
            frames = frames.float()
            frames = frames[:, :, target_width:, :]  # Crop to exo_width
            frames = frames.permute(0, 3, 1, 2).contiguous()
            return frames

    elif video_type == "ego_prior_GGA":
        target_width = height  # exo_width
        video_reader = decord.VideoReader(uri=video_path.as_posix(), width=target_width, height=height)
        video_num_frames = len(video_reader)
        
        if video_num_frames < max_num_frames:
            # Get all frames first
            frames = video_reader.get_batch(list(range(video_num_frames)))
            # Repeat the last frame until we reach max_num_frames
            last_frame = frames[-1:]
            num_repeats = max_num_frames - video_num_frames
            repeated_frames = last_frame.repeat(num_repeats, 1, 1, 1)
            frames = torch.cat([frames, repeated_frames], dim=0)
            frames = torch.ones_like(frames) * 255 / 2  # Set all frames to white
            return frames.float().permute(0, 3, 1, 2).contiguous()
        else:
            # Take first max_num_frames (no evenly downsampling)
            indices = list(range(max_num_frames))
            frames = video_reader.get_batch(indices)
            frames = frames.float()
            frames = torch.ones_like(frames) * 255 / 2 # Set all frames to white
            frames = frames.permute(0, 3, 1, 2).contiguous()
            return frames


    else:
        raise ValueError(f"Invalid video_type: {video_type}. Must be 'exo', 'ego_gt', or 'ego_prior'.")
    
   
def preprocess_video_with_buckets(
    video_path: Path,
    resolution_buckets: List[Tuple[int, int, int]],
) -> torch.Tensor:
    """
    Args:
        video_path: Path to the video file.
        resolution_buckets: List of tuples (num_frames, height, width) representing
            available resolution buckets.

    Returns:
        torch.Tensor: Video tensor with shape [F, C, H, W] where:
            F = number of frames
            C = number of channels (3 for RGB)
            H = height
            W = width

    The function processes the video through these steps:
        1. Finds nearest frame bucket <= video frame count
        2. Downsamples frames evenly to match bucket size
        3. Finds nearest resolution bucket based on dimensions
        4. Resizes frames to match bucket resolution
    """
    video_reader = decord.VideoReader(uri=video_path.as_posix())
    video_num_frames = len(video_reader)
    resolution_buckets = [bucket for bucket in resolution_buckets if bucket[0] <= video_num_frames]
    if len(resolution_buckets) == 0:
        raise ValueError(f"video frame count in {video_path} is less than all frame buckets {resolution_buckets}")

    nearest_frame_bucket = min(
        resolution_buckets,
        key=lambda bucket: video_num_frames - bucket[0],
        default=1,
    )[0]
    frame_indices = list(range(0, video_num_frames, video_num_frames // nearest_frame_bucket))
    frames = video_reader.get_batch(frame_indices)
    frames = frames[:nearest_frame_bucket].float()
    frames = frames.permute(0, 3, 1, 2).contiguous()

    nearest_res = min(resolution_buckets, key=lambda x: abs(x[1] - frames.shape[2]) + abs(x[2] - frames.shape[3]))
    nearest_res = (nearest_res[1], nearest_res[2])
    frames = torch.stack([resize(f, nearest_res) for f in frames], dim=0)

    return frames

if __name__ == "__main__":
    pointmap = generate_uniform_pointmap(height=288, width=512)
    print(pointmap.shape)
    imageio.imwrite("pointmap.png", (pointmap * 255).astype(np.uint8))
