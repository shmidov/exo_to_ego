{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed6fa1eb",
      "metadata": {},
      "source": [
        "# EgoX exo-to-ego (Colab) using local EgoX-main copy\n",
        "\n",
        "This notebook assumes you copied the EgoX repo into your Drive as `EgoX-main`.\n",
        "It will copy that folder into Colab, install dependencies, download checkpoints, prepare data, and run inference.\n",
        "\n",
        "**Prereqs you must provide**:\n",
        "- A Drive folder with `EgoX-main/` (the repo you copied locally).\n",
        "- Your input video at `data/raw/exo.mp4` inside your Drive workspace.\n",
        "- Depth maps, camera intrinsics, and ego camera extrinsics from EgoPriorRenderer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f86693",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "WORKDIR = '/content/drive/MyDrive/exo_to_ego'\n",
        "REPO_SRC = f'{WORKDIR}/EgoX-main'\n",
        "REPO_DST = '/content/EgoX'\n",
        "DATASET_DIR = f'{WORKDIR}/datasets/custom_exo'\n",
        "TAKE_NAME = 'take_001'\n",
        "INPUT_VIDEO = f'{WORKDIR}/data/raw/exo.mp4'\n",
        "OUTPUT_DIR = f'{WORKDIR}/outputs/egox'\n",
        "CHECKPOINT_DIR = f'{WORKDIR}/checkpoints'\n",
        "\n",
        "print('WORKDIR:', WORKDIR)\n",
        "print('REPO_SRC:', REPO_SRC)\n",
        "print('INPUT_VIDEO:', INPUT_VIDEO)\n",
        "print('DATASET_DIR:', DATASET_DIR)\n",
        "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
        "print('CHECKPOINT_DIR:', CHECKPOINT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282193a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy your local EgoX-main into Colab\n",
        "!rm -rf {REPO_DST}\n",
        "!cp -R {REPO_SRC} {REPO_DST}\n",
        "%cd /content/EgoX\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb87051",
      "metadata": {},
      "source": [
        "## Install dependencies\n",
        "EgoX expects Python 3.10 + CUDA 12.1. Colab usually already has CUDA, but we install PyTorch explicitly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cefea32c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA 12.1\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install EgoX deps\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e84fd29",
      "metadata": {},
      "source": [
        "## Download checkpoints\n",
        "This pulls the Wan2.1 pretrained model and the EgoX LoRA weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c7fbf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p {CHECKPOINT_DIR}\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Wan2.1 pretrained model\n",
        "!python - <<'PY'\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\n",
        "    repo_id='Wan-AI/Wan2.1-I2V-14B-480P-Diffusers',\n",
        "    local_dir='./checkpoints/pretrained_model/Wan2.1-I2V-14B-480P-Diffusers'\n",
        ")\n",
        "PY\n",
        "\n",
        "# EgoX LoRA weights\n",
        "!python - <<'PY'\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\n",
        "    repo_id='DAVIAN-Robotics/EgoX',\n",
        "    local_dir='./checkpoints/EgoX',\n",
        "    allow_patterns='*.safetensors'\n",
        ")\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab20275e",
      "metadata": {},
      "source": [
        "## Prepare dataset folder\n",
        "EgoX expects videos and depth maps in a specific structure. We create the folder and place your exo video there.\n",
        "It also expects 784x448 resolution and 49 frames. You should preprocess your video before inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26809c6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(f'{DATASET_DIR}/videos/{TAKE_NAME}', exist_ok=True)\n",
        "!cp {INPUT_VIDEO} {DATASET_DIR}/videos/{TAKE_NAME}/exo_raw.mp4\n",
        "\n",
        "# Resize + trim to 49 frames at 784x448 (30 fps). Adjust as needed.\n",
        "!ffmpeg -y -i {DATASET_DIR}/videos/{TAKE_NAME}/exo_raw.mp4 -vf 'scale=784:448,fps=30' -frames:v 49 {DATASET_DIR}/videos/{TAKE_NAME}/exo.mp4\n",
        "\n",
        "!ls -la {DATASET_DIR}/videos/{TAKE_NAME}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "166568d8",
      "metadata": {},
      "source": [
        "## Create meta.json\n",
        "This initializes a meta file with default camera extrinsics and ego intrinsics.\n",
        "You will later fill in camera intrinsics and ego camera extrinsics (from EgoPriorRenderer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff038676",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/EgoX/meta_init.py --folder_path {DATASET_DIR} --output_json {DATASET_DIR}/meta.json --overwrite\n",
        "!sed -n '1,200p' {DATASET_DIR}/meta.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd5ee64",
      "metadata": {},
      "source": [
        "## Generate captions (prompt)\n",
        "EgoX uses `caption.py` to generate prompts with GPT-4o. You must set the API base URL and key in `caption.py`.\n",
        "If you skip this, you can also write a custom prompt manually in a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6217e5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edit caption.py to set YOUR_BASE_URL and YOUR_API_KEY\n",
        "!sed -n '1,60p' /content/EgoX/caption.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14cda7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run caption generation (requires 49 frame images; follow EgoPriorRenderer or your own extractor)\n",
        "# This updates meta.json with prompt text\n",
        "# !python /content/EgoX/caption.py --json_file {DATASET_DIR}/meta.json --output_json {DATASET_DIR}/meta.json --overwrite\n",
        "\n",
        "# If you want a manual prompt instead, create a prompt file and skip caption.py:\n",
        "# with open(f'{DATASET_DIR}/prompt.txt','w') as f: f.write('your prompt here')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09054df",
      "metadata": {},
      "source": [
        "## Generate depth maps + ego camera extrinsics\n",
        "EgoX requires depth maps and ego camera extrinsics. Use EgoPriorRenderer for this step.\n",
        "Follow the official instructions: https://github.com/kdh8156/EgoX-EgoPriorRenderer\n",
        "After running it, your dataset should include:\n",
        "- `depth_maps/{TAKE_NAME}/frame_000.npy` ... `frame_048.npy`\n",
        "- `videos/{TAKE_NAME}/ego_Prior.mp4`\n",
        "- Updated `meta.json` with camera_intrinsics and ego_extrinsics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2850946",
      "metadata": {},
      "source": [
        "## Build list files for inference\n",
        "EgoX `infer.py` expects 3 list files: prompts, exo video paths, ego prior video paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2769413d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "meta_path = Path(f'{DATASET_DIR}/meta.json')\n",
        "meta = json.loads(meta_path.read_text())\n",
        "entry = meta['test_datasets'][0]\n",
        "\n",
        "prompt_text = entry.get('prompt', '').strip()\n",
        "if not prompt_text:\n",
        "    prompt_text = 'A person moves naturally in a room. Generate a plausible egocentric view.'\n",
        "\n",
        "prompt_list = f'{DATASET_DIR}/prompt_list.txt'\n",
        "exo_list = f'{DATASET_DIR}/exo_list.txt'\n",
        "ego_list = f'{DATASET_DIR}/ego_list.txt'\n",
        "\n",
        "Path(prompt_list).write_text(prompt_text + '\\n')\n",
        "Path(exo_list).write_text(entry['exo_path'] + '\\n')\n",
        "Path(ego_list).write_text(entry['ego_prior_path'] + '\\n')\n",
        "\n",
        "print('Wrote:', prompt_list)\n",
        "print('Wrote:', exo_list)\n",
        "print('Wrote:', ego_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae671e1",
      "metadata": {},
      "source": [
        "## Run inference\n",
        "Make sure depth maps and meta.json are complete before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8278b039",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p {OUTPUT_DIR}\n",
        "\n",
        "!python /content/EgoX/infer.py \\\n",
        "  --prompt {DATASET_DIR}/prompt_list.txt \\\n",
        "  --exo_video_path {DATASET_DIR}/exo_list.txt \\\n",
        "  --ego_prior_video_path {DATASET_DIR}/ego_list.txt \\\n",
        "  --meta_data_file {DATASET_DIR}/meta.json \\\n",
        "  --depth_root {DATASET_DIR}/depth_maps \\\n",
        "  --model_path /content/EgoX/checkpoints/pretrained_model/Wan2.1-I2V-14B-480P-Diffusers \\\n",
        "  --lora_path /content/EgoX/checkpoints/EgoX/pytorch_lora_weights.safetensors \\\n",
        "  --lora_rank 256 \\\n",
        "  --out {OUTPUT_DIR} \\\n",
        "  --seed 42 \\\n",
        "  --use_GGA \\\n",
        "  --cos_sim_scaling_factor 3.0\n",
        "\n",
        "!ls -la {OUTPUT_DIR}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
